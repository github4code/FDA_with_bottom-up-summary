# FM_Attention_with_bottom-up-summary

FM_Attention (focus masking attention) with bottom-up summarization is based on bottom-up-summary by Sebastian Gehrmann from Harvard. 
It is a fine-tuned version with its bottom-up attention mechanism on both word and sentence levels. <br>
The results show that both word-level weight adjustment and sentence-level attention enhancement can improve the summary generation performance.


### Here is the brief introduction for our work 

Website : https://bit.ly/2VQLuxg

[![Watch the video](https://img.youtube.com/vi/xw_xsnhuUig/maxresdefault.jpg)](https://www.youtube.com/watch?v=xw_xsnhuUig)


