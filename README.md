# FM_Attention_with_bottom-up-summary

The idea of FM_Attention (focus masking attention) is based on [bottom-up-summary](https://arxiv.org/abs/1808.10792) by [Sebastian Gehrmann
](https://scholar.harvard.edu/gehrmann/home). <br>
It's a fine-tuned version with bottom-up attention mechanism on both word and sentence levels. <br>
The result also shows that both word-level reweighting and sentence-level enhancement get improvement respectively. 

The dataset is the [CNN/DailyMail](https://cs.nyu.edu/~kcho/DMQA/) dataset. 

<img src="https://i.imgur.com/yNQMibc.png" width="75%" height="60%">

### The dataset pre-processing references the methods by following work
1. [NEUSUM](https://github.com/magic282/cnndm_acl18) by [QingyuZhou](https://res.qyzhou.me/)
2. [Pointer-Generator](https://github.com/abisee/pointer-generator) by [Abigail See](https://cs.stanford.edu/people/abisee/)

## Here is the brief introduction for our work 

- Website : https://bit.ly/2VQLuxg
- Click for the video :
[![Watch the video](https://img.youtube.com/vi/xw_xsnhuUig/maxresdefault.jpg)](https://www.youtube.com/watch?v=xw_xsnhuUig)

