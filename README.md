# FM_Attention_with_bottom-up-summary

FM_Attention (focus masking attention) is based on bottom-up-summary by Sebastian Gehrmann from Harvard. <br>
It's a fine-tuned version with bottom-up attention mechanism on both word and sentence levels. <br>
The result also shows that both word-level reweighting and sentence-level enhancement get improvement respectively. 

- The dataset is the [CNN/DailyMail](https://cs.nyu.edu/~kcho/DMQA/) dataset. 

<img src="https://i.imgur.com/yNQMibc.png" width="75%" height="60%">

### Here is the brief introduction for our work 

Website : https://bit.ly/2VQLuxg

[![Watch the video](https://img.youtube.com/vi/xw_xsnhuUig/maxresdefault.jpg)](https://www.youtube.com/watch?v=xw_xsnhuUig)

